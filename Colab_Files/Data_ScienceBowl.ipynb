{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DMW_Project_Data_ScienceBowl.ipynb","provenance":[{"file_id":"19IFL4P5yclH69PDNIJrPw3t5lVQDykGL","timestamp":1618258870688}],"collapsed_sections":[],"authorship_tag":"ABX9TyM4Osnn0GQp5JFogC3UsDRw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XuGYxHG9DK8O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618261797923,"user_tz":-330,"elapsed":40010,"user":{"displayName":"RAJAT SHARMA","photoUrl":"","userId":"13976417869986616281"}},"outputId":"16d0035f-41f6-4329-8508-73bc81c957b7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_2-uWF4ezP6k"},"source":["import os\n","import random\n","import numpy as np\n","import cv2\n","from tqdm import tqdm\n","from glob import glob\n","import tifffile as tif\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.losses import binary_crossentropy\n","import json\n","from sklearn.utils import shuffle\n","from tensorflow.keras.utils import CustomObjectScope\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.applications import *\n","from tensorflow.keras.models import Model\n","import sys\n","import warnings\n","from itertools import chain\n","from skimage.io import imread, imshow, imread_collection, concatenate_images\n","from skimage.transform import resize\n","from skimage.morphology import label\n","from keras.utils import Progbar\n","from tensorflow.keras.callbacks import *\n","from tensorflow.keras.optimizers import Adam, Nadam\n","from tensorflow.keras.metrics import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdBMQexBDoAH"},"source":["smooth = 1e-15\n","\n","def dice_coef(y_true, y_pred):\n","    y_true = tf.keras.layers.Flatten()(y_true)\n","    y_pred = tf.keras.layers.Flatten()(y_pred)\n","    intersection = tf.reduce_sum(y_true * y_pred)\n","    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n","\n","def dice_loss(y_true, y_pred):\n","    return 1.0 - dice_coef(y_true, y_pred)\n","\n","def iou(y_true, y_pred):\n","    def f(y_true, y_pred):\n","        intersection = (y_true * y_pred).sum()\n","        union = y_true.sum() + y_pred.sum() - intersection\n","        x = (intersection + smooth) / (union + smooth)\n","        x = x.astype(np.float32)\n","        return x\n","    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n","\n","def bce_dice_loss(y_true, y_pred):\n","    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n","\n","def focal_loss(y_true, y_pred):\n","    alpha=0.25\n","    gamma=2\n","    def focal_loss_with_logits(logits, targets, alpha, gamma, y_pred):\n","        weight_a = alpha * (1 - y_pred) ** gamma * targets\n","        weight_b = (1 - alpha) * y_pred ** gamma * (1 - targets)\n","        return (tf.math.log1p(tf.exp(-tf.abs(logits))) + tf.nn.relu(-logits)) * (weight_a + weight_b) + logits * weight_b\n","\n","    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n","    logits = tf.math.log(y_pred / (1 - y_pred))\n","    loss = focal_loss_with_logits(logits=logits, targets=y_true, alpha=alpha, gamma=gamma, y_pred=y_pred)\n","    return tf.reduce_mean(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fag_h4N-Dn4b"},"source":["def squeeze_excite_block(inputs, ratio=8):\n","    init = inputs\n","    channel_axis = -1\n","    filters = init.shape[channel_axis]\n","    se_shape = (1, 1, filters)\n","\n","    se = GlobalAveragePooling2D()(init)\n","    se = Reshape(se_shape)(se)\n","    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n","    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n","\n","    x = Multiply()([init, se])\n","    return x\n","\n","def conv_block(inputs, filters):\n","    x = inputs\n","\n","    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = squeeze_excite_block(x)\n","\n","    return x\n","\n","def encoder1(inputs):\n","    skip_connections = []\n","\n","    model = VGG19(include_top=False, weights='imagenet', input_tensor=inputs)\n","    names = [\"block1_conv2\", \"block2_conv2\", \"block3_conv4\", \"block4_conv4\"]\n","    for name in names:\n","        skip_connections.append(model.get_layer(name).output)\n","\n","    output = model.get_layer(\"block5_conv4\").output\n","    return output, skip_connections\n","\n","def decoder1(inputs, skip_connections):\n","    num_filters = [256, 128, 64, 32]\n","    skip_connections.reverse()\n","    x = inputs\n","\n","    for i, f in enumerate(num_filters):\n","        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n","        x = Concatenate()([x, skip_connections[i]])\n","        x = conv_block(x, f)\n","\n","    return x\n","\n","def encoder2(inputs):\n","    num_filters = [32, 64, 128, 256]\n","    skip_connections = []\n","    x = inputs\n","\n","    for i, f in enumerate(num_filters):\n","        x = conv_block(x, f)\n","        skip_connections.append(x)\n","        x = MaxPool2D((2, 2))(x)\n","\n","    return x, skip_connections\n","\n","def decoder2(inputs, skip_1, skip_2):\n","    num_filters = [256, 128, 64, 32]\n","    skip_2.reverse()\n","    x = inputs\n","\n","    for i, f in enumerate(num_filters):\n","        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n","        x = Concatenate()([x, skip_1[i], skip_2[i]])\n","        x = conv_block(x, f)\n","\n","    return x\n","\n","def output_block(inputs):\n","    x = Conv2D(1, (1, 1), padding=\"same\")(inputs)\n","    x = Activation('sigmoid')(x)\n","    return x\n","\n","def Upsample(tensor, size):\n","    def _upsample(x, size):\n","        return tf.image.resize(images=x, size=size)\n","    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\n","\n","def ASPP(x, filter):\n","    shape = x.shape\n","\n","    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n","    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n","    y1 = BatchNormalization()(y1)\n","    y1 = Activation(\"relu\")(y1)\n","    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\n","\n","    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n","    y2 = BatchNormalization()(y2)\n","    y2 = Activation(\"relu\")(y2)\n","\n","    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n","    y3 = BatchNormalization()(y3)\n","    y3 = Activation(\"relu\")(y3)\n","\n","    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n","    y4 = BatchNormalization()(y4)\n","    y4 = Activation(\"relu\")(y4)\n","\n","    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n","    y5 = BatchNormalization()(y5)\n","    y5 = Activation(\"relu\")(y5)\n","\n","    y = Concatenate()([y1, y2, y3, y4, y5])\n","\n","    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n","    y = BatchNormalization()(y)\n","    y = Activation(\"relu\")(y)\n","\n","    return y\n","\n","def build_model(shape):\n","    inputs = Input(shape)\n","    x, skip_1 = encoder1(inputs)\n","    x = ASPP(x, 64)\n","    x = decoder1(x, skip_1)\n","    outputs1 = output_block(x)\n","\n","    x = inputs * outputs1\n","\n","    x, skip_2 = encoder2(x)\n","    x = ASPP(x, 64)\n","    x = decoder2(x, skip_1, skip_2)\n","    outputs2 = output_block(x)\n","    outputs = Concatenate()([outputs1, outputs2])\n","\n","    model = Model(inputs, outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ox55TN88Dn2U"},"source":["def create_dir(path):\n","    try:\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","    except OSError:\n","        print(f\"Error: creating directory with name {path}\")\n","\n","def read_data(x, y):\n","    image = cv2.imread(x, cv2.IMREAD_COLOR)\n","    mask = cv2.imread(y, cv2.IMREAD_COLOR)\n","    return image, mask\n","\n","def read_params():\n","    with open(\"params.json\", \"r\") as f:\n","        data = f.read()\n","        params = json.loads(data)\n","        return params\n","\n","def load_data(path):\n","    images_path = os.path.join(path, \"image/*\")\n","    masks_path  = os.path.join(path, \"mask/*\")\n","\n","    images = glob(images_path)\n","    masks  = glob(masks_path)\n","\n","    return images, masks\n","\n","def shuffling(x, y):\n","    x, y = shuffle(x, y, random_state=42)\n","    return x, y\n","\n","def load_model_weight(path):\n","    with CustomObjectScope({'dice_loss': dice_loss, 'dice_coef': dice_coef, 'bce_dice_loss': bce_dice_loss, 'focal_loss': focal_loss, 'iou': iou}):\n","        model = load_model(path)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xucGJ-CTGIOP"},"source":["def prepare_data(path, num, img_height=192, img_width=256, img_channel=3):\n","  image_ids = next(os.walk(path + \"/stage1_train/\"))[1]\n","  X_array = np.zeros((num, img_height, img_width, img_channel), dtype=np.uint8)\n","  Y_array = np.zeros((num, img_height, img_width, 1), dtype=np.uint8)\n","\n","  print(\"Preparation of data -----------------\")\n","  if os.path.isfile(path + \"/files/images.npy\") and os.path.isfile(path + \"/files/masks.npy\"):\n","    print(\"Importing from Drive ------------\")\n","    X_array = np.load(path + \"/files/images.npy\")\n","    Y_array = np.load(path + \"/files/masks.npy\")\n","    return X_array, Y_array\n","  \n","  ap = Progbar(num)\n","  val = 0\n","  for n, ids in enumerate(image_ids):\n","    location = path + \"/stage1_train/\" + ids\n","    pic = imread(location + \"/images/\" + ids + \".png\")[:, :, :img_channel]\n","    pic = resize(pic, (img_height, img_width), mode=\"constant\", preserve_range=True)\n","    X_array[val] = pic\n","    mask = np.zeros((img_height, img_width, 1), dtype=np.bool)\n","    for mas in next(os.walk(location + \"/masks/\"))[2]:\n","      tmp = imread(location + \"/masks/\" + mas)\n","      tmp = np.expand_dims(resize(tmp, (img_height, img_width), mode=\"constant\", preserve_range=True), axis=-1)\n","      mask = np.maximum(mask, tmp)\n","    Y_array[val] = mask\n","    ap.update(val+1)\n","    val += 1\n","    if val >= num:\n","      break\n","  create_dir(path + \"/files/\")\n","  np.save(path + \"/files/images.npy\", X_array)\n","  np.save(path + \"/files/masks.npy\", Y_array)\n","  return X_array, Y_array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NwQyD6kqDnpQ","executionInfo":{"status":"ok","timestamp":1618262325669,"user_tz":-330,"elapsed":17368,"user":{"displayName":"RAJAT SHARMA","photoUrl":"","userId":"13976417869986616281"}},"outputId":"fa6178bc-fc4c-455c-9439-428c079d0fcc"},"source":["random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","image_path = \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data3\"\n","X_array, Y_array = prepare_data(image_path, 350)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Preparation of data -----------------\n","Importing from Drive ------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GfoC36oOFVQI"},"source":["def read_image(image):\n","    image = image/255.0\n","    image = image.astype(np.float32)\n","    return image\n","\n","def read_mask(mask):\n","    mask = mask.astype(np.float32)\n","    return mask\n","\n","def parse_data(x, y):\n","    def _parse(x, y):\n","        x = read_image(x)\n","        y = read_mask(y)\n","        y = np.concatenate([y, y], axis=-1)\n","        return x, y\n","\n","    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n","    x.set_shape([192, 256, 3])\n","    y.set_shape([192, 256, 2])\n","    return x, y\n","\n","def tf_dataset(x, y, batch=8):\n","    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n","    dataset = dataset.shuffle(buffer_size=32)\n","    dataset = dataset.map(map_func=parse_data)\n","    dataset = dataset.repeat()\n","    dataset = dataset.batch(batch)\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GBr1wN3PDwyL"},"source":["length = len(X_array)\n","\n","test_split = int(length * 0.25)\n","valid_split = int(length * 0.20)\n","\n","train_x, test_x = train_test_split(X_array, test_size=test_split, random_state=42)\n","train_y, test_y = train_test_split(Y_array, test_size=test_split, random_state=42)\n","\n","train_x, valid_x = train_test_split(train_x, test_size=valid_split, random_state=42)\n","train_y, valid_y = train_test_split(train_y, test_size=valid_split, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oh1N1HHJFVNw","executionInfo":{"status":"ok","timestamp":1618262644538,"user_tz":-330,"elapsed":232697,"user":{"displayName":"RAJAT SHARMA","photoUrl":"","userId":"13976417869986616281"}},"outputId":"2f64d216-4731-44a9-9989-7adad53d1919"},"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","model_path = \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data3/files/model.h5\"\n","batch_size = 13\n","epochs = 10\n","lr = 1e-4\n","shape = (192, 256, 3)\n","\n","model = build_model(shape)\n","metrics = [\"acc\", dice_coef, iou, Recall(), Precision()]\n","\n","train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n","valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n","\n","model.compile(loss=dice_loss, optimizer=Nadam(lr), metrics=metrics)\n","\n","callbacks = [ModelCheckpoint(model_path), ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20), CSVLogger(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data3/files/training_process.csv\"), TensorBoard(), EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False)]\n","train_steps = (len(train_x)//batch_size)\n","valid_steps = (len(valid_x)//batch_size)\n","\n","if len(train_x) % batch_size != 0:\n","    train_steps += 1\n","\n","if len(valid_x) % batch_size != 0:\n","    valid_steps += 1\n","\n","model.fit(train_dataset, epochs=epochs, validation_data=valid_dataset, steps_per_epoch=train_steps, validation_steps=valid_steps, callbacks=callbacks, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 1s 0us/step\n","Epoch 1/10\n","15/15 [==============================] - 78s 1s/step - loss: -0.2368 - acc: 0.8608 - dice_coef: 1.2368 - iou: 1.8147 - recall: 0.7972 - precision: 0.2207 - val_loss: 0.0713 - val_acc: 0.0288 - val_dice_coef: 0.9287 - val_iou: 0.8670 - val_recall: 0.4831 - val_precision: 0.3164\n","Epoch 2/10\n","15/15 [==============================] - 12s 833ms/step - loss: -0.7201 - acc: 0.8728 - dice_coef: 1.7201 - iou: 6.5328 - recall: 0.9872 - precision: 0.4080 - val_loss: -0.0514 - val_acc: 0.1447 - val_dice_coef: 1.0514 - val_iou: 1.1099 - val_recall: 0.5537 - val_precision: 0.6312\n","Epoch 3/10\n","15/15 [==============================] - 13s 844ms/step - loss: -0.8243 - acc: 0.8916 - dice_coef: 1.8243 - iou: 10.7883 - recall: 0.9959 - precision: 0.4490 - val_loss: -0.1694 - val_acc: 0.6555 - val_dice_coef: 1.1694 - val_iou: 1.4155 - val_recall: 0.7045 - val_precision: 0.6170\n","Epoch 4/10\n","15/15 [==============================] - 13s 852ms/step - loss: -0.8635 - acc: 0.9031 - dice_coef: 1.8635 - iou: 13.8943 - recall: 0.9983 - precision: 0.4831 - val_loss: -0.2330 - val_acc: 0.7738 - val_dice_coef: 1.2330 - val_iou: 1.6193 - val_recall: 0.7751 - val_precision: 0.6100\n","Epoch 5/10\n","15/15 [==============================] - 13s 863ms/step - loss: -0.8835 - acc: 0.8530 - dice_coef: 1.8835 - iou: 16.6672 - recall: 0.9984 - precision: 0.4987 - val_loss: -0.2975 - val_acc: 0.8380 - val_dice_coef: 1.2975 - val_iou: 1.8702 - val_recall: 0.8552 - val_precision: 0.6071\n","Epoch 6/10\n","15/15 [==============================] - 13s 874ms/step - loss: -0.9013 - acc: 0.7620 - dice_coef: 1.9013 - iou: 19.7824 - recall: 0.9985 - precision: 0.5163 - val_loss: -0.3637 - val_acc: 0.8703 - val_dice_coef: 1.3637 - val_iou: 2.1837 - val_recall: 0.9472 - val_precision: 0.6141\n","Epoch 7/10\n","15/15 [==============================] - 13s 886ms/step - loss: -0.9013 - acc: 0.7263 - dice_coef: 1.9013 - iou: 20.3859 - recall: 0.9971 - precision: 0.5291 - val_loss: -0.3988 - val_acc: 0.8879 - val_dice_coef: 1.3988 - val_iou: 2.4162 - val_recall: 0.9441 - val_precision: 0.6202\n","Epoch 8/10\n","15/15 [==============================] - 13s 874ms/step - loss: -0.9132 - acc: 0.6905 - dice_coef: 1.9132 - iou: 22.4369 - recall: 0.9989 - precision: 0.5420 - val_loss: -0.5160 - val_acc: 0.9028 - val_dice_coef: 1.5160 - val_iou: 3.1873 - val_recall: 0.9857 - val_precision: 0.5697\n","Epoch 9/10\n","15/15 [==============================] - 13s 871ms/step - loss: -0.9163 - acc: 0.7230 - dice_coef: 1.9163 - iou: 23.2851 - recall: 0.9991 - precision: 0.5328 - val_loss: -0.5818 - val_acc: 0.9503 - val_dice_coef: 1.5818 - val_iou: 3.8237 - val_recall: 0.9881 - val_precision: 0.5804\n","Epoch 10/10\n","15/15 [==============================] - 13s 873ms/step - loss: -0.9239 - acc: 0.6019 - dice_coef: 1.9239 - iou: 25.9193 - recall: 0.9991 - precision: 0.5312 - val_loss: -0.6403 - val_acc: 0.9523 - val_dice_coef: 1.6403 - val_iou: 4.7810 - val_recall: 0.9914 - val_precision: 0.5829\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f2d00037090>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"mPiMb8HqXJ5m"},"source":["def read_image_(x):\n","    image = x\n","    image = np.clip(image - np.median(image)+127, 0, 255)\n","    image = image/255.0\n","    image = image.astype(np.float32)\n","    image = np.expand_dims(image, axis=0)\n","    return image\n","\n","def read_mask_(y):\n","    mask = y\n","    mask = mask.astype(np.float32)\n","    mask = mask/255.0\n","    mask = np.expand_dims(mask, axis=-1)\n","    return mask\n","\n","def mask_to_3d(mask):\n","    mask = np.squeeze(mask)\n","    mask = [mask, mask, mask]\n","    mask = np.transpose(mask, (1, 2, 0))\n","    return mask\n","\n","def parse(y_pred):\n","    y_pred = np.expand_dims(y_pred, axis=-1)\n","    y_pred = y_pred[..., -1]\n","    y_pred = y_pred.astype(np.float32)\n","    y_pred = np.expand_dims(y_pred, axis=-1)\n","    return y_pred\n","\n","def evaluate_normal(model, x_data, y_data):\n","    THRESHOLD = 0.5\n","    total = []\n","    for i, (x, y) in tqdm(enumerate(zip(x_data, y_data)), total=len(x_data)):\n","        x = read_image_(x)\n","        y = read_mask_(y)\n","        _, h, w, _ = x.shape\n","\n","        y_pred1 = parse(model.predict(x)[0][..., -2])\n","        y_pred2 = parse(model.predict(x)[0][..., -1])\n","        \n","        line = np.ones((h, 10, 3)) * 255.0\n","        \n","        all_images = [ x[0] * 255.0, line, mask_to_3d(y) * 255.0, line, mask_to_3d(y_pred1) * 255.0, line, mask_to_3d(y_pred2) * 255.0 ]\n","        mask = np.concatenate(all_images, axis=1)\n","        cv2.imwrite(f\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data3/files/resulting_images/{i}.png\", mask)\n","\n","smooth = 1."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vkr1SFaYXJ2T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618262759672,"user_tz":-330,"elapsed":41090,"user":{"displayName":"RAJAT SHARMA","photoUrl":"","userId":"13976417869986616281"}},"outputId":"4af7e814-2884-4bbc-b9a4-67d7781cfbdf"},"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","create_dir(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data3/files/resulting_images/\")\n","batch_size = 6\n","\n","test_dataset = tf_dataset(test_x, test_y, batch=batch_size)\n","test_steps = (len(test_x)//batch_size)\n","\n","if len(test_x) % batch_size != 0:\n","    test_steps += 1\n","\n","model = load_model_weight(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data3/files/model.h5\")\n","lr = 1e-4\n","metrics = [ \"acc\", dice_coef, iou, Recall(), Precision() ]\n","model.compile(loss=dice_loss, optimizer=Nadam(lr), metrics=metrics)\n","model.evaluate(test_dataset, steps=test_steps)\n","evaluate_normal(model, test_x, test_y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15/15 [==============================] - 9s 131ms/step - loss: -0.6092 - acc: 0.9476 - dice_coef: 1.6092 - iou: 4.3848 - recall_1: 0.9936 - precision_1: 0.5698\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 87/87 [00:17<00:00,  4.94it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"t3QUpVY-uO2W"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4yAGPHV4LWe"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQuFmHfHuO01"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nn4iDgbluOyt"},"source":[""],"execution_count":null,"outputs":[]}]}