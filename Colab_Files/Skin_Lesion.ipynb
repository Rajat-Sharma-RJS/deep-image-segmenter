{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DMW_Project_Skin_Lesion.ipynb","provenance":[{"file_id":"1SjT2NihknwXTljSuxPIjwmp-KD5vcjck","timestamp":1618078280962}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"h-f3b6hBsAWL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618081783562,"user_tz":-330,"elapsed":33385,"user":{"displayName":"RAJAT SHARMA","photoUrl":"","userId":"13976417869986616281"}},"outputId":"2f95353a-5633-4f7c-be52-ea4c9ae56d21"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xNEhiCOkKr8y"},"source":["import os\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","from glob import glob\n","import tifffile as tif\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.losses import binary_crossentropy\n","import json\n","from sklearn.utils import shuffle\n","from tensorflow.keras.utils import CustomObjectScope\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.applications import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import *\n","from tensorflow.keras.optimizers import Adam, Nadam\n","from tensorflow.keras.metrics import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWBuTdQcBGHu","executionInfo":{"status":"ok","timestamp":1618081804380,"user_tz":-330,"elapsed":9362,"user":{"displayName":"RAJAT SHARMA","photoUrl":"","userId":"13976417869986616281"}},"outputId":"a66b5ede-b4c8-4f24-b283-b51519fce275"},"source":["pip install -U albumentations"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting albumentations\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\n","\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n","\u001b[?25hCollecting opencv-python-headless>=4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/6d/92f377bece9b0ec9c893081dbe073a65b38d7ac12ef572b8f70554d08760/opencv_python_headless-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (37.6MB)\n","\u001b[K     |████████████████████████████████| 37.6MB 80kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.16.2)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n","Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n","Collecting imgaug>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n","\u001b[K     |████████████████████████████████| 952kB 45.2MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n","Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n","Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n","Installing collected packages: opencv-python-headless, imgaug, albumentations\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","  Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-0.5.2 imgaug-0.4.0 opencv-python-headless-4.5.1.48\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-WYzhMPjBP4t"},"source":["import cv2\n","from albumentations import HorizontalFlip, VerticalFlip, CenterCrop, Crop, Transpose, ChannelShuffle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4hEdv-lktjWH"},"source":["smooth = 1e-15\n","\n","def dice_coef(y_true, y_pred):\n","    y_true = tf.keras.layers.Flatten()(y_true)\n","    y_pred = tf.keras.layers.Flatten()(y_pred)\n","    intersection = tf.reduce_sum(y_true * y_pred)\n","    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n","\n","def dice_loss(y_true, y_pred):\n","    return 1.0 - dice_coef(y_true, y_pred)\n","\n","def iou(y_true, y_pred):\n","    def f(y_true, y_pred):\n","        intersection = (y_true * y_pred).sum()\n","        union = y_true.sum() + y_pred.sum() - intersection\n","        x = (intersection + smooth) / (union + smooth)\n","        x = x.astype(np.float32)\n","        return x\n","    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n","\n","def bce_dice_loss(y_true, y_pred):\n","    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n","\n","def focal_loss(y_true, y_pred):\n","    alpha=0.25\n","    gamma=2\n","    def focal_loss_with_logits(logits, targets, alpha, gamma, y_pred):\n","        weight_a = alpha * (1 - y_pred) ** gamma * targets\n","        weight_b = (1 - alpha) * y_pred ** gamma * (1 - targets)\n","        return (tf.math.log1p(tf.exp(-tf.abs(logits))) + tf.nn.relu(-logits)) * (weight_a + weight_b) + logits * weight_b\n","\n","    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n","    logits = tf.math.log(y_pred / (1 - y_pred))\n","    loss = focal_loss_with_logits(logits=logits, targets=y_true, alpha=alpha, gamma=gamma, y_pred=y_pred)\n","    return tf.reduce_mean(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUqPm3NxwNLH"},"source":["def squeeze_excite_block(inputs, ratio=8):\n","    init = inputs\n","    channel_axis = -1\n","    filters = init.shape[channel_axis]\n","    se_shape = (1, 1, filters)\n","\n","    se = GlobalAveragePooling2D()(init)\n","    se = Reshape(se_shape)(se)\n","    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n","    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n","\n","    x = Multiply()([init, se])\n","    return x\n","\n","def conv_block(inputs, filters):\n","    x = inputs\n","\n","    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = squeeze_excite_block(x)\n","\n","    return x\n","\n","def encoder1(inputs):\n","    skip_connections = []\n","\n","    model = VGG19(include_top=False, weights='imagenet', input_tensor=inputs)\n","    names = [\"block1_conv2\", \"block2_conv2\", \"block3_conv4\", \"block4_conv4\"]\n","    for name in names:\n","        skip_connections.append(model.get_layer(name).output)\n","\n","    output = model.get_layer(\"block5_conv4\").output\n","    return output, skip_connections\n","\n","def decoder1(inputs, skip_connections):\n","    num_filters = [256, 128, 64, 32]\n","    skip_connections.reverse()\n","    x = inputs\n","\n","    for i, f in enumerate(num_filters):\n","        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n","        x = Concatenate()([x, skip_connections[i]])\n","        x = conv_block(x, f)\n","\n","    return x\n","\n","def encoder2(inputs):\n","    num_filters = [32, 64, 128, 256]\n","    skip_connections = []\n","    x = inputs\n","\n","    for i, f in enumerate(num_filters):\n","        x = conv_block(x, f)\n","        skip_connections.append(x)\n","        x = MaxPool2D((2, 2))(x)\n","\n","    return x, skip_connections\n","\n","def decoder2(inputs, skip_1, skip_2):\n","    num_filters = [256, 128, 64, 32]\n","    skip_2.reverse()\n","    x = inputs\n","\n","    for i, f in enumerate(num_filters):\n","        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n","        x = Concatenate()([x, skip_1[i], skip_2[i]])\n","        x = conv_block(x, f)\n","\n","    return x\n","\n","def output_block(inputs):\n","    x = Conv2D(1, (1, 1), padding=\"same\")(inputs)\n","    x = Activation('sigmoid')(x)\n","    return x\n","\n","def Upsample(tensor, size):\n","    def _upsample(x, size):\n","        return tf.image.resize(images=x, size=size)\n","    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\n","\n","def ASPP(x, filter):\n","    shape = x.shape\n","\n","    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n","    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n","    y1 = BatchNormalization()(y1)\n","    y1 = Activation(\"relu\")(y1)\n","    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\n","\n","    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n","    y2 = BatchNormalization()(y2)\n","    y2 = Activation(\"relu\")(y2)\n","\n","    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n","    y3 = BatchNormalization()(y3)\n","    y3 = Activation(\"relu\")(y3)\n","\n","    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n","    y4 = BatchNormalization()(y4)\n","    y4 = Activation(\"relu\")(y4)\n","\n","    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n","    y5 = BatchNormalization()(y5)\n","    y5 = Activation(\"relu\")(y5)\n","\n","    y = Concatenate()([y1, y2, y3, y4, y5])\n","\n","    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n","    y = BatchNormalization()(y)\n","    y = Activation(\"relu\")(y)\n","\n","    return y\n","\n","def build_model(shape):\n","    inputs = Input(shape)\n","    x, skip_1 = encoder1(inputs)\n","    x = ASPP(x, 64)\n","    x = decoder1(x, skip_1)\n","    outputs1 = output_block(x)\n","\n","    x = inputs * outputs1\n","\n","    x, skip_2 = encoder2(x)\n","    x = ASPP(x, 64)\n","    x = decoder2(x, skip_1, skip_2)\n","    outputs2 = output_block(x)\n","    outputs = Concatenate()([outputs1, outputs2])\n","\n","    model = Model(inputs, outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsBs6BwJxIXE"},"source":["def create_dir(path):\n","    try:\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","    except OSError:\n","        print(f\"Error: creating directory with name {path}\")\n","\n","def read_data(x, y):\n","    image = cv2.imread(x, cv2.IMREAD_COLOR)\n","    mask = cv2.imread(y, cv2.IMREAD_COLOR)\n","    return image, mask\n","\n","def read_params():\n","    with open(\"params.json\", \"r\") as f:\n","        data = f.read()\n","        params = json.loads(data)\n","        return params\n","\n","def load_data(path):\n","    images_path = os.path.join(path, \"image/*\")\n","    masks_path  = os.path.join(path, \"mask/*\")\n","\n","    images = glob(images_path)\n","    masks  = glob(masks_path)\n","\n","    return images, masks\n","\n","def shuffling(x, y):\n","    x, y = shuffle(x, y, random_state=42)\n","    return x, y\n","\n","def load_model_weight(path):\n","    with CustomObjectScope({'dice_loss': dice_loss, 'dice_coef': dice_coef, 'bce_dice_loss': bce_dice_loss, 'focal_loss': focal_loss, 'iou': iou}):\n","        model = load_model(path)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Btpq8_t-0oHy"},"source":["def augment_data(images, masks, save_path, augment=True):\n","    crop_size = (192-32, 256-32)\n","    size = (256, 192)\n","\n","    for image, mask in tqdm(zip(images, masks), total=len(images)):\n","        image_name = image.split(\"/\")[-1].split(\".\")[0]\n","        mask_name = mask.split(\"/\")[-1].split(\".\")[0]\n","\n","        x, y = read_data(image, mask)\n","        try:\n","            h, w, c = x.shape\n","        except Exception as e:\n","            image = image[:-1]\n","            x, y = read_data(image, mask)\n","            h, w, c = x.shape\n","\n","        if augment == True:\n","            ## Transpose\n","            aug = Transpose(p=1)\n","            augmented = aug(image=x, mask=y)\n","            x2 = augmented['image']\n","            y2 = augmented['mask']\n","\n","            ## Channel Shuffle\n","            aug = ChannelShuffle(p=1)\n","            augmented = aug(image=x, mask=y)\n","            x3 = augmented['image']\n","            y3 = augmented['mask']\n","\n","            images = [x, x2, x3]\n","            masks  = [y, y2, y3]\n","        else:\n","            images = [x]\n","            masks  = [y]\n","\n","        idx = 0\n","        for i, m in zip(images, masks):\n","            i = cv2.resize(i, size)\n","            m = cv2.resize(m, size)\n","\n","            tmp_image_name = f\"{image_name}_{idx}.jpg\"\n","            tmp_mask_name  = f\"{mask_name}_{idx}.jpg\"\n","\n","            image_path = os.path.join(save_path, \"image/\", tmp_image_name)\n","            mask_path  = os.path.join(save_path, \"mask/\", tmp_mask_name)\n","\n","            cv2.imwrite(image_path, i)\n","            cv2.imwrite(mask_path, m)\n","\n","            idx += 1\n","\n","def get_data(path, split=0.1):\n","    train_x = glob(os.path.join(path, \"trainx/*\"))\n","    train_y = glob(os.path.join(path, \"trainy/*\"))\n","\n","    valid_x = glob(os.path.join(path, \"validationx/*\"))\n","    valid_y = glob(os.path.join(path, \"validationy/*\"))\n","\n","    test_x = glob(os.path.join(path, \"testx/*\"))\n","    test_y = glob(os.path.join(path, \"testy/*\"))\n","\n","    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VqmZZesT5Ofh","executionInfo":{"status":"ok","timestamp":1618082987203,"user_tz":-330,"elapsed":349474,"user":{"displayName":"RAJAT SHARMA","photoUrl":"","userId":"13976417869986616281"}},"outputId":"ec095935-f564-4686-d7cd-ef47e7ae319e"},"source":["np.random.seed(42)\n","path = \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/skin_lesion_segmentation\"\n","(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = get_data(path, split=0.1)\n","\n","create_dir(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/train/image/\")\n","create_dir(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/train/mask/\")\n","create_dir(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/valid/image/\")\n","create_dir(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/valid/mask/\")\n","create_dir(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/test/image/\")\n","create_dir(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/test/mask/\")\n","\n","augment_data(train_x, train_y, \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/train/\", augment=True)\n","augment_data(valid_x, valid_y, \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/valid/\", augment=False)\n","augment_data(test_x, test_y, \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/test/\", augment=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 185/185 [03:19<00:00,  1.08s/it]\n","100%|██████████| 80/80 [00:55<00:00,  1.43it/s]\n","100%|██████████| 114/114 [01:15<00:00,  1.51it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"E16wVXqP9px9"},"source":["def read_image(x):\n","    x = x.decode()\n","    image = cv2.imread(x, cv2.IMREAD_COLOR)\n","    image = np.clip(image - np.median(image)+127, 0, 255)\n","    image = image/255.0\n","    image = image.astype(np.float32)\n","    return image\n","\n","def read_mask(y):\n","    y = y.decode()\n","    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n","    mask = mask/255.0\n","    mask = mask.astype(np.float32)\n","    mask = np.expand_dims(mask, axis=-1)\n","    return mask\n","\n","def parse_data(x, y):\n","    def _parse(x, y):\n","        x = read_image(x)\n","        y = read_mask(y)\n","        y = np.concatenate([y, y], axis=-1)\n","        return x, y\n","\n","    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n","    x.set_shape([192, 256, 3])\n","    y.set_shape([192, 256, 2])\n","    return x, y\n","\n","def tf_dataset(x, y, batch=8):\n","    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n","    dataset = dataset.shuffle(buffer_size=32)\n","    dataset = dataset.map(map_func=parse_data)\n","    dataset = dataset.repeat()\n","    dataset = dataset.batch(batch)\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OKIPpxsi92WE","executionInfo":{"status":"ok","timestamp":1618083645862,"user_tz":-330,"elapsed":475095,"user":{"displayName":"RAJAT SHARMA","photoUrl":"","userId":"13976417869986616281"}},"outputId":"4495c7d8-5eba-4e08-99a1-c7425524c918"},"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","create_dir(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/files\")\n","\n","train_path = \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/train\"\n","valid_path = \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/valid\"\n","\n","train_x = sorted(glob(os.path.join(train_path, \"image\", \"*.jpg\")))\n","train_y = sorted(glob(os.path.join(train_path, \"mask\", \"*.jpg\")))\n","train_x, train_y = shuffling(train_x, train_y)\n","\n","valid_x = sorted(glob(os.path.join(valid_path, \"image\", \"*.jpg\")))\n","valid_y = sorted(glob(os.path.join(valid_path, \"mask\", \"*.jpg\")))\n","\n","model_path = \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/files/model.h5\"\n","batch_size = 15\n","epochs = 10\n","lr = 1e-4\n","shape = (192, 256, 3)\n","\n","model = build_model(shape)\n","metrics = [ \"acc\", dice_coef, iou, Recall(), Precision() ]\n","\n","train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n","valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n","model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)\n","\n","callbacks = [ ModelCheckpoint(model_path), ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20), CSVLogger(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/files/data.csv\"), TensorBoard(), EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False) ]\n","\n","train_steps = (len(train_x)//batch_size)\n","valid_steps = (len(valid_x)//batch_size)\n","\n","if len(train_x) % batch_size != 0:\n","    train_steps += 1\n","\n","if len(valid_x) % batch_size != 0:\n","    valid_steps += 1\n","\n","model.fit(train_dataset, epochs=epochs, validation_data=valid_dataset, steps_per_epoch=train_steps, validation_steps=valid_steps, callbacks=callbacks, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 0s 0us/step\n","Epoch 1/10\n","37/37 [==============================] - 91s 1s/step - loss: 0.5686 - acc: 0.8369 - dice_coef: 0.4314 - iou: 0.2790 - recall: 0.7651 - precision: 0.4218 - val_loss: 0.5762 - val_acc: 0.1358 - val_dice_coef: 0.4238 - val_iou: 0.2694 - val_recall: 0.3093 - val_precision: 0.9682\n","Epoch 2/10\n","37/37 [==============================] - 36s 980ms/step - loss: 0.3787 - acc: 0.8240 - dice_coef: 0.6213 - iou: 0.4521 - recall: 0.9307 - precision: 0.7408 - val_loss: 0.4495 - val_acc: 0.2272 - val_dice_coef: 0.5505 - val_iou: 0.3804 - val_recall: 0.6719 - val_precision: 0.9605\n","Epoch 3/10\n","37/37 [==============================] - 37s 983ms/step - loss: 0.3405 - acc: 0.8749 - dice_coef: 0.6595 - iou: 0.4940 - recall: 0.9347 - precision: 0.8267 - val_loss: 0.4150 - val_acc: 0.9245 - val_dice_coef: 0.5850 - val_iou: 0.4142 - val_recall: 0.6586 - val_precision: 0.9754\n","Epoch 4/10\n","37/37 [==============================] - 37s 992ms/step - loss: 0.3178 - acc: 0.8695 - dice_coef: 0.6822 - iou: 0.5199 - recall: 0.9352 - precision: 0.8744 - val_loss: 0.3686 - val_acc: 0.9836 - val_dice_coef: 0.6314 - val_iou: 0.4631 - val_recall: 0.8051 - val_precision: 0.9322\n","Epoch 5/10\n","37/37 [==============================] - 37s 996ms/step - loss: 0.3093 - acc: 0.8444 - dice_coef: 0.6907 - iou: 0.5305 - recall: 0.9331 - precision: 0.8827 - val_loss: 0.3531 - val_acc: 0.9397 - val_dice_coef: 0.6469 - val_iou: 0.4804 - val_recall: 0.8667 - val_precision: 0.8733\n","Epoch 6/10\n","37/37 [==============================] - 37s 998ms/step - loss: 0.2968 - acc: 0.8108 - dice_coef: 0.7032 - iou: 0.5455 - recall: 0.9389 - precision: 0.8970 - val_loss: 0.3530 - val_acc: 0.9124 - val_dice_coef: 0.6470 - val_iou: 0.4789 - val_recall: 0.7745 - val_precision: 0.9623\n","Epoch 7/10\n","37/37 [==============================] - 37s 1s/step - loss: 0.2753 - acc: 0.7684 - dice_coef: 0.7247 - iou: 0.5704 - recall: 0.9386 - precision: 0.9163 - val_loss: 0.3231 - val_acc: 0.9875 - val_dice_coef: 0.6769 - val_iou: 0.5137 - val_recall: 0.8551 - val_precision: 0.9295\n","Epoch 8/10\n","37/37 [==============================] - 38s 1s/step - loss: 0.2760 - acc: 0.7349 - dice_coef: 0.7240 - iou: 0.5713 - recall: 0.9346 - precision: 0.9119 - val_loss: 0.3180 - val_acc: 0.9979 - val_dice_coef: 0.6820 - val_iou: 0.5191 - val_recall: 0.8273 - val_precision: 0.9478\n","Epoch 9/10\n","37/37 [==============================] - 38s 1s/step - loss: 0.2601 - acc: 0.7041 - dice_coef: 0.7399 - iou: 0.5890 - recall: 0.9332 - precision: 0.9200 - val_loss: 0.3041 - val_acc: 0.9894 - val_dice_coef: 0.6959 - val_iou: 0.5342 - val_recall: 0.7949 - val_precision: 0.9416\n","Epoch 10/10\n","37/37 [==============================] - 38s 1s/step - loss: 0.2503 - acc: 0.6859 - dice_coef: 0.7497 - iou: 0.6010 - recall: 0.9292 - precision: 0.9314 - val_loss: 0.3009 - val_acc: 0.9805 - val_dice_coef: 0.6991 - val_iou: 0.5385 - val_recall: 0.9272 - val_precision: 0.8396\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fbf0e0bd610>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"iC_T1X6rA-WK"},"source":["def read_image_(x):\n","    image = cv2.imread(x, cv2.IMREAD_COLOR)\n","    image = np.clip(image - np.median(image)+127, 0, 255)\n","    image = image/255.0\n","    image = image.astype(np.float32)\n","    image = np.expand_dims(image, axis=0)\n","    return image\n","\n","def read_mask_(y):\n","    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n","    mask = mask.astype(np.float32)\n","    mask = mask/255.0\n","    mask = np.expand_dims(mask, axis=-1)\n","    return mask\n","\n","def mask_to_3d(mask):\n","    mask = np.squeeze(mask)\n","    mask = [mask, mask, mask]\n","    mask = np.transpose(mask, (1, 2, 0))\n","    return mask\n","\n","def parse(y_pred):\n","    y_pred = np.expand_dims(y_pred, axis=-1)\n","    y_pred = y_pred[..., -1]\n","    y_pred = y_pred.astype(np.float32)\n","    y_pred = np.expand_dims(y_pred, axis=-1)\n","    return y_pred\n","\n","def evaluate_normal(model, x_data, y_data):\n","    THRESHOLD = 0.5\n","    total = []\n","    for i, (x, y) in tqdm(enumerate(zip(x_data, y_data)), total=len(x_data)):\n","        x = read_image_(x)\n","        y = read_mask_(y)\n","        _, h, w, _ = x.shape\n","\n","        y_pred1 = parse(model.predict(x)[0][..., -2])\n","        y_pred2 = parse(model.predict(x)[0][..., -1])\n","        \n","        line = np.ones((h, 10, 3)) * 255.0\n","        \n","        all_images = [ x[0] * 255.0, line, mask_to_3d(y) * 255.0, line, mask_to_3d(y_pred1) * 255.0, line, mask_to_3d(y_pred2) * 255.0 ]\n","        mask = np.concatenate(all_images, axis=1)\n","        cv2.imwrite(f\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/files/results/{i}.png\", mask)\n","\n","smooth = 1."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23bU6_c5-ZHG","executionInfo":{"status":"ok","timestamp":1618083893463,"user_tz":-330,"elapsed":47892,"user":{"displayName":"RAJAT SHARMA","photoUrl":"","userId":"13976417869986616281"}},"outputId":"8901334f-0518-4552-81ba-ed34a6503f49"},"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","create_dir(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/files/results/\")\n","\n","batch_size = 8\n","\n","test_path = \"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/new_data/test\"\n","test_x = sorted(glob(os.path.join(test_path, \"image\", \"*.jpg\")))\n","test_y = sorted(glob(os.path.join(test_path, \"mask\", \"*.jpg\")))\n","test_dataset = tf_dataset(test_x, test_y, batch=batch_size)\n","\n","test_steps = (len(test_x)//batch_size)\n","if len(test_x) % batch_size != 0:\n","    test_steps += 1\n","\n","model = load_model_weight(\"/content/drive/MyDrive/SEM6/DMW/DMW_Project/data/aesthetic/files/model.h5\")\n","\n","lr = 1e-4\n","metrics = [ \"acc\", dice_coef, iou, Recall(), Precision() ]\n","model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)\n","model.evaluate(test_dataset, steps=test_steps)\n","evaluate_normal(model, test_x, test_y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15/15 [==============================] - 11s 234ms/step - loss: 0.2990 - acc: 0.9763 - dice_coef: 0.7010 - iou: 0.5425 - recall_1: 0.9283 - precision_1: 0.8023\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 114/114 [00:23<00:00,  4.88it/s]\n"],"name":"stderr"}]}]}